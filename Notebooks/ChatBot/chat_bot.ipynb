{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Загружаем переменные окружения из .env (если требуется)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r9wh0x750xq4v5z33ylwrzw40000gn/T/ipykernel_58409/1634373640.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "/Users/sergey/Desktop/Voise_RAG/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Загрузка векторного хранилища (ChromaDB) из сохранённого каталога\n",
    "# Здесь используются те же параметры, что применялись при загрузке данных\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "knowledge_vector_store = Chroma(\n",
    "    collection_name=\"knowledge_markdown\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db/knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание ретривера по схожести из векторного хранилища\n",
    "retriever_similarity = knowledge_vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечение всех документов из коллекции для построения BM25 ретривера\n",
    "# Для этого используем внутренний метод ._collection.get() для доступа к сохранённым данным\n",
    "raw_collection = knowledge_vector_store._collection.get()\n",
    "docs = []\n",
    "# В raw_collection ожидаются ключи \"documents\" и \"metadatas\"\n",
    "for content, meta in zip(raw_collection[\"documents\"], raw_collection[\"metadatas\"]):\n",
    "    docs.append(Document(page_content=content, metadata=meta))\n",
    "\n",
    "# Создание BM25 ретривера из списка документов\n",
    "keyword_retriever = BM25Retriever.from_documents(docs)\n",
    "keyword_retriever.k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединение ретриверов через EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[retriever_similarity, keyword_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergey/Desktop/Voise_RAG/Notebooks/openrouter_loader.py:15: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# Загрузка модели OpenRouter через модуль openrouter_loader\n",
    "from openrouter_loader import load_model\n",
    "\n",
    "# Загружаем модель (например, GPT-3.5-turbo) с нужными параметрами\n",
    "generate_response = load_model(model_name=\"deepseek/deepseek-r1-distill-llama-8b\", temperature=0.5, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример функции для генерации ответа с использованием RAG (предполагается, что функция generate_answer уже определена)\n",
    "def generate_answer(question: str, history: list = None) -> str:\n",
    "    \"\"\"\n",
    "    Генерирует ответ на вопрос пользователя, используя комбинированный поиск по базе знаний.\n",
    "    \n",
    "    Шаги:\n",
    "      1. Получение релевантных документов (ensemble_retriever.get_relevant_documents вызывает предупреждение о депрекации).\n",
    "      2. Формирование контекста из найденных фрагментов.\n",
    "      3. Составление prompt с контекстом и историей диалога (если предоставлена).\n",
    "      4. Вызов LLM через OpenRouter для генерации ответа.\n",
    "    \"\"\"\n",
    "    # Получение релевантных документов (обратите внимание, что метод get_relevant_documents устарел)\n",
    "    relevant_docs = ensemble_retriever.get_relevant_documents(question)\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    if history:\n",
    "        history_text = \"\\n\".join(history)\n",
    "        prompt = (\n",
    "            \"Тебе предоставлены следующие данные из базы знаний:\\n\\n\"\n",
    "            f\"{context_text}\\n\\n\"\n",
    "            \"История диалога:\\n\"\n",
    "            f\"{history_text}\\n\\n\"\n",
    "            \"Используй эти данные для ответа на вопрос. Если информации недостаточно, сообщи об этом.\\n\\n\"\n",
    "            f\"Вопрос: {question}\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            \"Тебе предоставлены следующие данные из базы знаний:\\n\\n\"\n",
    "            f\"{context_text}\\n\\n\"\n",
    "            \"Используй эти данные для ответа на вопрос. Если информации недостаточно, сообщи об этом.\\n\\n\"\n",
    "            f\"Вопрос: {question}\"\n",
    "        )\n",
    "    \n",
    "    # Вызов функции генерации ответа\n",
    "    answer = generate_response(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало чата. Для выхода введите 'х'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/r9wh0x750xq4v5z33ylwrzw40000gn/T/ipykernel_58409/663165022.py:13: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = ensemble_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Бот: Ошибка вызова модели: 'NoneType' object is not iterable \n",
      "\n",
      "Завершение чата.\n"
     ]
    }
   ],
   "source": [
    "# Интерактивный чат в Jupyter Notebook\n",
    "def chat():\n",
    "    print(\"Начало чата. Для выхода введите 'х'.\")\n",
    "    history = []\n",
    "    while True:\n",
    "        user_input = input(\"Пользователь: \")\n",
    "        if user_input.strip().lower() == 'х':\n",
    "            print(\"Завершение чата.\")\n",
    "            break\n",
    "        answer = generate_answer(user_input, history)\n",
    "        print(\"\\nБот:\", answer, \"\\n\")\n",
    "        # Обновляем историю диалога: можно сохранять последние N обменов\n",
    "        history.append(f\"Пользователь: {user_input}\")\n",
    "        history.append(f\"Бот: {answer}\")\n",
    "\n",
    "# Запуск чата\n",
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
